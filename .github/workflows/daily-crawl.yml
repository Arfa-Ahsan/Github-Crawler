name: Daily GitHub Crawler

on:
  schedule:
    - cron: "0 7 * * *" # Daily at 7 AM UTC
  workflow_dispatch: # Allows manual trigger
  push:
    branches: [main] # Run on push to test

jobs:
  crawl:
    runs-on: ubuntu-latest

    # Job-level environment variables (available to all steps)
    env:
      DB_HOST: 127.0.0.1
      DB_PORT: 5432
      DB_NAME: github_crawler
      DB_USER: postgres
      DB_PASSWORD: postgres
      PGPASSWORD: postgres   # ✅ ensures psql never asks for a password

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: github_crawler
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Cache Python dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Setup PostgreSQL - Create tables and schemas
        run: |
          echo "Waiting for PostgreSQL to be ready..."
          until pg_isready -h $DB_HOST -p $DB_PORT -U $DB_USER; do
            sleep 2
          done

          echo "Creating database schema..."
          psql -h $DB_HOST -U $DB_USER -d $DB_NAME -f database/schema.sql

          echo "Verifying tables created..."
          psql -h $DB_HOST -U $DB_USER -d $DB_NAME -c "\dt"

      - name: Crawl Stars - Fetch 100,000 GitHub repositories
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}  # ✅ default GitHub Actions token
        run: |
          echo "Starting crawler to fetch 100,000 repositories..."
          python main.py

          echo "Crawler completed. Checking database..."
          psql -h $DB_HOST -U $DB_USER -d $DB_NAME -c "SELECT COUNT(*) as total FROM repositories;"

      - name: Export database to CSV
        run: |
          echo "Exporting repositories to CSV..."
          psql -h $DB_HOST -U $DB_USER -d $DB_NAME -c "\COPY (SELECT * FROM repositories ORDER BY stars DESC) TO 'repositories.csv' WITH CSV HEADER;"

          echo "Exporting star history to CSV..."
          psql -h $DB_HOST -U $DB_USER -d $DB_NAME -c "\COPY (SELECT * FROM repository_star_history ORDER BY recorded_at DESC) TO 'star_history.csv' WITH CSV HEADER;"

          echo "Creating summary statistics..."
          psql -h $DB_HOST -U $DB_USER -d $DB_NAME -c "\COPY (
            SELECT 
              COUNT(*) as total_repositories,
              SUM(stars) as total_stars,
              AVG(stars) as avg_stars,
              MAX(stars) as max_stars,
              MIN(stars) as min_stars
            FROM repositories
          ) TO 'summary_stats.csv' WITH CSV HEADER;"

          echo "Files created:"
          ls -lh *.csv || true

      - name: Upload database dump as artifact
        uses: actions/upload-artifact@v4
        with:
          name: github-crawler-data-${{ github.run_number }}
          path: |
            repositories.csv
            star_history.csv
            summary_stats.csv
          retention-days: 30

      - name: Display summary
        run: |
          echo "=== Crawl Summary ==="
          if [ -f summary_stats.csv ]; then
            cat summary_stats.csv
          else
            echo "summary_stats.csv not found"
          fi

          echo ""
          echo "=== Top 10 Most Starred Repositories ==="
          if [ -f repositories.csv ]; then
            head -n 11 repositories.csv | column -t -s,
          else
            echo "repositories.csv not found"
          fi
